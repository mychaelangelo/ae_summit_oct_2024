### Beyond the Surface of Your Data: The Quest to Unlock its Full Potential

**Dr Jonathan Richard Schwarz (Head of AI Research at Thomson Reuters)**

***

* **Deep learning has shifted from architecture-focused to data-centric approaches** Around 2020-2021, the AI field moved away from primarily focusing on network arhcitures and algorithms. Instead, researchers started to emphasize the quality of training data and patterns within it. They discovered that a few proven algorithms, paired with smarter ways of selecting and organizing data, could achieve superior results.

* **Current large-scale training wastes resources due to how data naturally behaves.** Schwarz showed that adding more data yields diminishing returns on a logarithmic scale. Each doubling of training data brings increasingly smaller gains in model performance. This inefficiency becomes particularly clear in random sampling approaches, where AI systems frequently process redundant information instead of encountering truly novel examples. The vast majority of data ends up contributing only marginal improvements to the model's knowledge.

* **Research demonstrates that remarkable improvements in model performance are possible while using significantly less data:**
  - CLIP (Contrastive Language-Image Pre-training) models, which learn to understand both images and text by matching them together, achieved better performance with 90% less data
  - 7B parameter language models maintained or improved results while using 60% less data
  - Instruction tuning (the process of teaching AI models to follow specific instructions by training them on example tasks) achieved comparable results using only 5% of initial training data

* **Schwarz discussed a “foundational curricula” approach to overcome computational challenges.** Rather than experimenting with large, expensive AI models to find the best training approach, researchers first used smaller versions of the same AI model to discover the best order and timing for introducing different types of training data. They then applied these insights to train larger models. This cut computing costs by 25-46% while matching or exceeding previous performance.

* **Data-centric approaches can prevent models from forgetting skills.** When companies adapt AI models for specific domains (like law or medicine), these models often forget their general abilities. It's a bit like a universal translator that becomes highly fluent in medical terminology but suddenly loses its ability to translate everyday conversation. Schwarz's team found a solution: strategically remind the models of their previous training by carefully selecting and reusing key examples from their earlier training runs. This method preserved 81% of the model's original capabilities across different tasks, while traditional approaches only retained 26%, and sophisticated technical tweaks managed 70%.

* **The presentation emphasized the growing adoption of synthetic and hybrid data approaches.** Recent developments show a clear trend:
  - Nvidia reported that 98% of their model alignment data was artificially generated rather than collected from real-world sources
  - Meta repeatedly generates synthetic data to refine models
  - Google DeepMind acknowledged data quality as a crucial factor for high-performing models

* **Schwarz concluded by sharing some practical advice.**
  - Work with industry experts to create organized maps of what data to collect (for example, working with law professors to catalog different types of legal documents)
  - Have qualified experts, not random contractors, identify and label important information in the data
  - Continuously test the AI system against expert knowledge to find weak spots
  - Create additional training data that combines real examples with synthetic data
  - Add advanced AI techniques only after building a strong foundation of domain knowledge

### How Should We Fund AI Research? (Panel Session)

- **Dr Jakob Foerster** - Associate Professor (Oxford / Meta)
- **Dr Feryal Behbahani** - Research Scientist (DeepMind)
- **Katie Collins** - PhD Student (Cambridge & Alan Turing Institute)
- **Felix Sosa** - PhD Student (Harvard and MIT)
- **Dr David Balduzzi** - Machine Learning Researcher (XTX Markets)

***

* **The panel brought together diverse perspectives on AI's future trajectory, featuring experts from academia, industry, and research institutions.** This group included researchers and executives from DeepMind, Cambridge University, Meta's Fundamental AI Research team, XTX Markets, and Harvard/MIT. 

* **Current AI limitations and the need for fundamental shifts in approaches were discussed.** Some of the panel argued that while the current wave of large language models and self-supervised learning (where AI learns patterns without human labeling) has shown impressive results, it relies too heavily on massive datasets and computation. Future progress requires:
  - Learning algorithms that can succeed with significantly less data (training models on “2 trillion tokens” was considered unreasonably inefficient)
  - Moving beyond the pattern of simply collecting custom datasets to patch model limitations
  - Designing new architectures instead of tweaking existing language models
  - Understanding the principles of intelligence through compression (representing information efficiently) and generalization (applying learning to new situations) rather than massive-scale training

* **The discussion also highlighted the need to rethink AI evaluation methods.** Some of the panel criticized the current practice of evaluating billion-dollar AI projects using hastily created benchmarks. They proposed a number of specific improvements:
  - Evaluation metrics that test genuine generalization ability (how well AI applies learning to new situations) rather than just performance after extensive training
  - Assessments that measure performance in realistic scenarios (like operating a sewage plant) rather than idealized environments
  - Measuring success with limited data and computing power to mirror real-world constraints
  - Using cognitive science insights to assess capabilities like theory of mind (i.e. understanding the perspectives of others) and precise instruction following

* **One of the panelists urged the AI field to consider the quality and organization of data, rather than just volume.** The field's primary challenge lies in creating coherent, well-structured datasets, representing a shift from the “download the whole internet” approach to more carefully curated data collection across various scientific domains.

* **The panel discussed how traditional reinforcement learning (where an AI system learns from rewards and consequences) makes unrealistic assumptions.** Specifically:
  - Assuming perfect access to reward signals (i.e. perfect feedback about success or failure)
  - Presuming perfect models of environment dynamics (through Markov Decision Processes or MDPs, which are mathematical frameworks for modeling decision-making in situations where outcomes are partly random)
  - Expecting complete access to system internals (like being able to see and modify all parts of a system, rather than treating it as a black box)
These limitations have restricted reinforcement learning's success to primarily game environments rather than real-world applications.

* **The panel identified several promising research directions for the next five years:**
  - Probabilistic programming (creating AI systems that can reason about uncertainty)
  - Multi-agent systems (where multiple AI systems interact and collaborate) and human-AI collaboration
  - Open-ended learning (where AI systems can continuously learn and adapt to new situations)
  - Sample-efficient learning algorithms (that can learn from less data)
  - Better integration of cognitive science insights (understanding how human intelligence works)

* **A crucial insight emerged about the need to diversify research approaches.** The panel warned against the field's tendency to converge on similar approaches (particularly at large tech companies) and encouraged:
  - Investment in alternative modeling paradigms
  - Focus on problems beyond current industry priorities
  - Support for research programs that might not show immediate commercial promise
  - Better bridges between academic research and practical applications


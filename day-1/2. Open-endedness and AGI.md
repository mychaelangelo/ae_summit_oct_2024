### Open-Endedness and General Intelligence

**Professor Tim Rocktäschel (Senior Staff Research Scientist at Google DeepMind & Professor of AI at UCL)**

***

* **Rocktäschel established key definitions of artificial general intelligence (AGI) and open-endedness.** He referenced a [DeepMind paper](https://arxiv.org/abs/2311.02462) to outline AI capability levels from narrow to general intelligence. While we've achieved superhuman performance in specific domains like protein folding and chess, current language models (GPT, Claude, Gemini) show only “emerging“ general intelligence. This means they demonstrate competence across various non-physical tasks, including metacognitive abilities. To achieve true AGI, Rocktäschel argues we need open-ended learning.

* **Traditional and open-ended learning differ crucially.** Biological evolution and the [Picbreeder](https://dl.acm.org/doi/abs/10.1145/1357054.1357328) system demonstrate successful open-ended processes. Traditional machine learning uses well-defined loss or reward functions—like a teacher grading with specific criteria. In contrast, open-ended systems must:
  - Set their own problems and explore endlessly
  - Generate novelty that observers can't predict
  - Maintain learnability, allowing patterns to be discovered

* **AI learning approaches have undergone a recent paradigm shift.** Modern AI now uses “in-context learning“ within large language models—a technique where AI learns from examples provided in the immediate conversation—rather than focusing solely on parameter optimization. More capable models actually benefit more from techniques like chain-of-thought prompting, challenging the assumption that better models would need less guidance.

* **Rocktäschel's team has developed several innovative language model systems:**
  - [Promptbreeder](https://arxiv.org/abs/2309.16797): Uses evolutionary principles to create self-improving LLM prompts
  - [Rainbow Teaming](https://arxiv.org/abs/2402.16822): Generates diverse prompts to identify AI systems' weaknesses
  - [AI Debate](https://arxiv.org/abs/2402.06782): Shows humans make better decisions when evaluating two AIs in debate versus consulting one AI

* **Embodied AI faces specific challenges that require innovative solutions.** While video generation has progressed (Wave AI, Sora), current simulators lack the complexity needed for general intelligence. The [GENIE system](https://arxiv.org/abs/2402.15391) addresses this by learning interactive environments from video data without needing explicit action information during training.

* **Open-endedness will play an essential role in advancing AI capability.** Progress often requires unexpected intermediate steps. Both Picbreeder and Promptbreeder demonstrate this—seemingly random prompts like “Let's take a deep breath“ proved surprisingly effective. This suggests that improvement paths aren't always linear or intuitive.

* **Rocktäschel envisions that future AI advancement will need three key components:**
  - Foundational models with gradient-based learning—a mathematical technique where AI systems learn by gradually adjusting their parameters to reduce errors, similar to how you might find the bottom of a valley by always walking downhill
  - Some form of reinforcement learning—where AI learns through trial and error, receiving rewards for successful actions, much like how we train pets
  - Most importantly, open-ended loops of variation and selection that mirror scientific discovery—where AI systems can continuously generate new ideas, test them, and build upon successful ones, just as evolution and human science have done throughout history